{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae92aad0-17c9-4e5c-ac58-31ba1bb52ee3",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "93e71942-baf7-4a8d-b22e-d9afa5da5374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.sgd import SGD\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe127c5b-af98-41cf-b965-8360932a1763",
   "metadata": {},
   "source": [
    "dataset = MNIST(root=\"/home/paradox/Desktop/ai/pygrad\",download=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45a201b5-2698-4a5c-8104-89d3d449e586",
   "metadata": {},
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, height=28, weight=28, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(height*weight, height*weight),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(height*weight, height*weight),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(height*weight, height*weight),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(height*weight, n_classes),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e1cd17c9-d372-46b2-b81f-7841d9633748",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE=512\n",
    "HEIGHT=28\n",
    "WIDTH=28\n",
    "N_CLASSES = 10"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab4486f4-ac13-4b1b-8d4c-5c0bf8100b14",
   "metadata": {},
   "source": [
    "model = Model().to(DEVICE)\n",
    "\n",
    "train_data =  dataset.train_data.clone().float().reshape(-1, HEIGHT * WIDTH).to(DEVICE)\n",
    "train_labels = dataset.train_labels.clone().long().to(DEVICE)\n",
    "\n",
    "test_data = dataset.test_data.float().reshape(-1, HEIGHT * WIDTH).to(DEVICE)\n",
    "test_labels = dataset.test_labels.long().to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac1dffea-c04b-4a46-87bb-fca080367baf",
   "metadata": {},
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    accuracy = []\n",
    "    \n",
    "    for batch_idx in (pbar := tqdm(range(0, len(train_data), BATCH_SIZE))):\n",
    "\n",
    "        model.train()\n",
    "    \n",
    "    \n",
    "        x = train_data[batch_idx: batch_idx + BATCH_SIZE]\n",
    "        y = train_labels[batch_idx: batch_idx + BATCH_SIZE] \n",
    "    \n",
    "        logits = model(x)\n",
    "        loss = ce_loss(logits, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        training_loss.append(float(loss)) \n",
    "        \n",
    "    for batch_idx in (pbar := tqdm(range(0, len(test_data), BATCH_SIZE))):\n",
    "        \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.no_grad(): \n",
    "            x = test_data[batch_idx: batch_idx + BATCH_SIZE]\n",
    "            y = test_labels[batch_idx: batch_idx + BATCH_SIZE] \n",
    "        \n",
    "            logits = model(x)\n",
    "            loss = ce_loss(logits, y)\n",
    "            validation_loss.append(float(loss)) \n",
    "\n",
    "            preds = logits.softmax(dim = 1).argmax(dim=-1).cpu().numpy()\n",
    "            acc = accuracy_score(y.cpu().numpy(), preds)\n",
    "            accuracy.append(acc)\n",
    "            \n",
    "        pbar.set_description(f\"epoch: {epoch}, average_training_loss: {np.mean(training_loss):.4f}, average_val_loss: {np.mean(validation_loss)}, accuracy: {np.mean(accuracy)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc1313b-df6d-475b-a2ff-82f4344e0ef5",
   "metadata": {},
   "source": [
    "# PyGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9acbb9b4-a724-4068-91ab-cbb65ddd879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tensor():\n",
    "    def __init__(self, value, label=None, _childrens=set()):\n",
    "\n",
    "\n",
    "        if isinstance(value, np.ndarray):\n",
    "            self.value = value\n",
    "        else:\n",
    "            self.value = np.array(value)\n",
    "        self._childrens = _childrens\n",
    "        self.label = label\n",
    "        self._backwards = lambda: None\n",
    "        \n",
    "        if np.isscalar(value):\n",
    "            self.grad = np.zeros(1)\n",
    "        else:\n",
    "            self.grad = np.zeros(value.shape)\n",
    "\n",
    "        self.shape = self.value.shape\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.label == None:\n",
    "            return f\"Tensor(value={self.value}, grad={self.grad})\"\n",
    "        else:\n",
    "            return f\"Tensor(value={self.value}, label={self.label}, grad={self.grad})\"\n",
    "            \n",
    "    def __add__(self, other):\n",
    "       \n",
    "        t = Tensor(self.value + other.value, _childrens=set([self, other]))\n",
    "\n",
    "        def backwards():\n",
    "\n",
    "            if self.grad.shape == t.grad.shape:\n",
    "                self.grad += t.grad\n",
    "                \n",
    "            # Row Vector\n",
    "            elif self.grad.shape[-1] == t.grad.shape[-1]:\n",
    "                self.grad += t.grad.sum(0)\n",
    "                \n",
    "            # Column Vector\n",
    "            elif len(self.grad.shape) == 2 and self.grad.shape[-1] == 1:\n",
    "                self.grad += t.grad.sum(1)\n",
    "\n",
    "            # Scalar\n",
    "            else:\n",
    "                self.grad += t.grad.sum()\n",
    "                \n",
    "            if other.grad.shape == t.grad.shape:\n",
    "                other.grad += t.grad\n",
    "                \n",
    "            # Row Vector\n",
    "            elif self.grad.shape[-1] == t.grad.shape[-1]:\n",
    "                other.grad += t.grad.sum(0)\n",
    "                \n",
    "            # Column Vector\n",
    "            elif len(self.grad.shape) == 2 and self.grad.shape[-1] == 1:\n",
    "                other.grad += t.grad.sum(1)\n",
    "\n",
    "            # Scalar\n",
    "            else:\n",
    "                other.grad += t.grad.sum()\n",
    "\n",
    "\n",
    "        t._backwards = backwards\n",
    "        \n",
    "        return t\n",
    "        \n",
    "    def backward(self):\n",
    "\n",
    "        assert ((self.value.shape == () or self.value.shape == (1,)) and \"Grads can be created for scalar inputs only\")\n",
    "\n",
    "        if self.value.shape != ():\n",
    "            self.value = self.value[0]\n",
    "        \n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._childrens:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backwards()\n",
    "\n",
    "\n",
    "    def dot(self, other):\n",
    "\n",
    "        assert (self.shape[-1] == other.shape[0])\n",
    "        assert (len(self.shape) == 2 and \"currently only supports dot product for 2D tensors\")            \n",
    "        \n",
    "        t = Tensor(np.dot(self.value, other.value), _childrens=set([self, other]))\n",
    "\n",
    "        def backwards():\n",
    "            \n",
    "            self.grad += (other.value @ t.grad.T).T        \n",
    "            other.grad += (self.value.T @ t.grad)\n",
    "    \n",
    "        t._backwards = backwards\n",
    "        return t\n",
    "\n",
    "\n",
    "    # Scalar / Elementwise multiplication, we assume that both of the multiplicands have\n",
    "    # same dimensions.\n",
    "    def __mul__(self, other):\n",
    "\n",
    "        t = Tensor(self.value * other.value, _childrens=set([self, other]))\n",
    "\n",
    "        def backwards():\n",
    "            print(t.grad)\n",
    "\n",
    "            if self.grad.shape == t.grad.shape:\n",
    "                self.grad += other.value * t.grad\n",
    "\n",
    "            # Row Vector\n",
    "            elif self.grad.shape[-1] == t.grad.shape[-1]:\n",
    "                self.grad += (other.value * t.grad).sum(0)\n",
    "                \n",
    "            # Column Vector\n",
    "            elif len(self.grad.shape) == 2 and self.grad.shape[1] == 1:\n",
    "                self.grad += (other.value * t.grad).sum(1)\n",
    "\n",
    "            # Scalar\n",
    "            else:\n",
    "                self.grad += (other.value * t.grad).sum()\n",
    "                \n",
    "            if other.grad.shape == t.grad.shape:\n",
    "                other.grad += self.value * t.grad\n",
    "\n",
    "            # Row Vector\n",
    "            elif other.grad.shape[-1] == t.grad.shape[-1]:\n",
    "                other.grad += (self.value * t.grad).sum(0)\n",
    "                \n",
    "            # Column Vector\n",
    "            elif len(other.grad.shape) == 2 and other.grad.shape[1] == 1:\n",
    "                other.grad += (self.value * t.grad).sum(1)\n",
    "\n",
    "            # Scalar\n",
    "            else:\n",
    "                other.grad += (self.value * t.grad).sum()\n",
    "                \n",
    "            # other.grad += self.value * t.grad\n",
    "            \n",
    "        t._backwards = backwards\n",
    "        return t\n",
    "\n",
    "    def __pow__(self, n):\n",
    "\n",
    "        assert (isinstance(n, Tensor) == False)\n",
    "\n",
    "        t = Tensor(self.value ** n, _childrens=set([self]))\n",
    "\n",
    "        def backwards():\n",
    "            self.grad += n * (self.value ** (n - 1)) * t.grad\n",
    "\n",
    "        t._backwards = backwards\n",
    "        return t\n",
    "        \n",
    "\n",
    "    def relu(self):\n",
    "\n",
    "        n = np.copy(self.value)\n",
    "        n[n <= 0] = 0\n",
    "        t = Tensor(n, _childrens=set([self]))\n",
    "\n",
    "        def backwards():\n",
    "            self.grad[0 <= self.value] += t.grad[0 <= self.value]\n",
    "            \n",
    "        t._backwards = backwards\n",
    "        return t\n",
    "\n",
    "    def exp(self):\n",
    "\n",
    "        t = Tensor(np.exp(self.value), _childrens=set([self]))\n",
    "        \n",
    "        def backwards():\n",
    "            self.grad += t.value * t.grad\n",
    "\n",
    "        t._backwards = backwards\n",
    "        return t\n",
    "\n",
    "    def __truediv__(self,other):\n",
    "        return self * (other ** -1)\n",
    "\n",
    "    def sum(self):\n",
    "        t = Tensor(self.value.sum(), _childrens=set([self]))\n",
    "\n",
    "        def backwards():\n",
    "            self.grad += t.grad\n",
    "            \n",
    "        t._backwards = backwards\n",
    "        \n",
    "        return t\n",
    "\n",
    "    def max(self, dim):\n",
    "\n",
    "        assert (dim == 0 or dim == 1) and \"Nanograd only supports 2D Tensors\"\n",
    "\n",
    "        t = Tensor(self.value.max(dim), _childrens=set([self]))\n",
    "\n",
    "        def backwards():\n",
    "\n",
    "            # self.value is 1D Tensor\n",
    "            if t.value.shape == ():\n",
    "                self.grad[self.value == t.value] += t.grad\n",
    "                return\n",
    "\n",
    "            row,cols = self.value.shape\n",
    "            if dim == 0:\n",
    "\n",
    "                if len(t.grad.shape) == 1:\n",
    "                    self.grad[self.value == self.value.max(0)] += t.grad\n",
    "                else:\n",
    "                    self.grad[self.value == self.value.max(0)] += t.grad[self.value == self.value.max(0)]\n",
    "            else:\n",
    "                \n",
    "\n",
    "                if len(t.grad.shape) == 1:\n",
    "                    self.grad[self.value == self.value.max(1).reshape(-1,1)] += t.grad\n",
    "                else:\n",
    "                    self.grad[self.value == self.value.max(1).reshape(-1,1)] += t.grad[self.value == self.value.max(1).reshape(-1,1)]\n",
    "\n",
    "\n",
    "        t._backwards = backwards\n",
    "        return t\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f884e999-2ea4-4bd2-a13b-19c1b135282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xTr = np.random.randn(16, HEIGHT * WIDTH) / ((HEIGHT * WIDTH) ** 0.5)\n",
    "w1 = np.random.randn(HEIGHT * WIDTH, HEIGHT * WIDTH) / ((HEIGHT * WIDTH) ** 0.5)\n",
    "b1 = np.random.randn(HEIGHT * WIDTH) / ((HEIGHT * WIDTH) ** 0.5)\n",
    "w2 = np.random.randn(HEIGHT * WIDTH, N_CLASSES) / ((HEIGHT * WIDTH) ** 0.5)\n",
    "b2 = np.random.randn(N_CLASSES) / (N_CLASSES ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5e75607e-e131-42fe-827d-759f3bb7bad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape () doesn't match the broadcast shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[184], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# n_probs = (n_logits.exp() / n_logits.exp().sum()).max(dim=1).sum() / batch_size\u001b[39;00m\n\u001b[1;32m     18\u001b[0m n_probs \u001b[38;5;241m=\u001b[39m (n_b2 \u001b[38;5;241m/\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m---> 20\u001b[0m \u001b[43mn_probs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[178], line 87\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(topo):\n\u001b[0;32m---> 87\u001b[0m     \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backwards\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[178], line 157\u001b[0m, in \u001b[0;36mTensor.__pow__.<locals>.backwards\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackwards\u001b[39m():\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape () doesn't match the broadcast shape (1,)"
     ]
    }
   ],
   "source": [
    "n_xTr = Tensor(xTr)\n",
    "\n",
    "n_w1 = Tensor(w1)\n",
    "n_b1 = Tensor(b1)\n",
    "\n",
    "n_w2 = Tensor(w2)\n",
    "n_b2 = Tensor(b2)\n",
    "\n",
    "batch_size = Tensor(np.array(16.0))\n",
    "\n",
    "# Forward Pass\n",
    "\n",
    "n_h1 = (n_xTr.dot(n_w1) + n_b1).relu()\n",
    "n_logits = n_h1.dot(n_w2) + n_b2\n",
    "\n",
    "# n_probs = (n_logits.exp() / n_logits.exp().sum()).max(dim=1).sum() / batch_size\n",
    "\n",
    "n_probs = (n_b2 / batch_size).sum()\n",
    "\n",
    "n_probs.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "771668ae-1f3d-46be-b859-3a9bfdfdb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_xTr = torch.tensor(xTr)\n",
    "\n",
    "p_w1 = torch.tensor(w1); p_w1.requires_grad = True\n",
    "p_b1 = torch.tensor(b1); p_b1.requires_grad = True\n",
    "\n",
    "p_w2 = torch.tensor(w2); p_w2.requires_grad = True\n",
    "p_b2 = torch.tensor(b2); p_b2.requires_grad = True\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "p_h1 = (p_xTr.matmul(p_w1) + p_b1).relu()\n",
    "p_logits = p_h1.matmul(p_w2) + p_b\n",
    "\n",
    "p_probs = (p_logits.exp() / p_logits.exp().sum() ).max(1).values.sum()\n",
    "\n",
    "p_probs.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "557e4b6c-bbaf-45ff-92b9-639089805f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "parameters = [(n_w1, p_w1), (n_b1, p_b1), (n_w2, p_w2), (n_b2, p_b2)]\n",
    "\n",
    "\n",
    "for n,p in parameters:\n",
    "    print(np.all(np.abs(n.grad - p.grad.numpy()) < 1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b25163e7-2876-4752-9a50-9824a3253fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(value=0.10875924192675124, grad=1),\n",
       " tensor(0.1088, dtype=torch.float64, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_probs, p_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a128b36d-4449-4742-a9a6-c8a44c41c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pygrad():\n",
    "\n",
    "    n_xTr = Tensor(xTr)\n",
    "    n_w1 = Tensor(w1)\n",
    "    n_b1 = Tensor(b1)\n",
    "    n_w2 = Tensor(w2)\n",
    "    \n",
    "    h1_preact = n_xTr.dot(n_w1) + n_b1\n",
    "    h1 = h1_preact.relu()\n",
    "    o = h1.dot(n_w2).sum()\n",
    "\n",
    "    o.backwards()\n",
    "    return n_w1,n_w2\n",
    "\n",
    "n_w1,n_w2 = pygrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3a0a97a2-268d-4665-a943-2e790f445e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_xTr = torch.tensor(xTr)\n",
    "p_w1 = torch.tensor(w1); p_w1.requires_grad = True\n",
    "p_w2 = torch.tensor(w2); p_w2.requires_grad = True\n",
    "\n",
    "o = p_xTr.matmul(p_w1).relu().matmul(p_w2).sum()\n",
    "\n",
    "o.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a521ebc4-9655-4a2f-9d7a-99fe5de827b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grads of matrix n_w1 is same\n",
      "Grads of matrix n_w2 is same\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if np.all(np.abs(p_w1.grad.numpy() - n_w1.grad) < 1e-8):\n",
    "    print(f\"Grads of matrix n_w1 is same\")\n",
    "else:\n",
    "    print(\"Grads of matrix n_w1 is not same\")\n",
    "\n",
    "if np.all(np.abs(p_w2.grad.numpy() - n_w2.grad) < 1e-8):\n",
    "    print(f\"Grads of matrix n_w2 is same\")\n",
    "else:\n",
    "    print(\"Grads of matrix n_w2 is not same\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d7631-8966-448a-ac4d-2d9a07155651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
